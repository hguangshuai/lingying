# å¸¸ç”¨æŸå¤±å‡½æ•°

## é—®é¢˜ï¼šå¸¸ç”¨çš„æŸå¤±å‡½æ•°æœ‰å“ªäº›ï¼Ÿå®ƒä»¬çš„å…¬å¼ã€è¾“å‡ºèŒƒå›´ã€å¥½å¤„å’Œç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ

### ä¸€ã€è¯¦ç»†ä¸­æ–‡è§£é‡Š

æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œæ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡ã€‚ä¸åŒçš„æŸå¤±å‡½æ•°æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œé€‚ç”¨äºä¸åŒçš„ä»»åŠ¡å’Œåœºæ™¯ã€‚

#### 1. å›å½’æŸå¤±å‡½æ•°

##### 1.1 MSEï¼ˆMean Squared Errorï¼Œå‡æ–¹è¯¯å·®ï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{MSE}}$ï¼šå‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼‰
- $n$ï¼šæ ·æœ¬æ•°é‡ï¼ˆNumber of samplesï¼‰
- $y_i $ï¼šç¬¬ $ i $ ä¸ªæ ·æœ¬çš„çœŸå®å€¼ï¼ˆTrue value of the $ i$-th sampleï¼‰
- $\hat{y}_i $ï¼šç¬¬ $ i $ ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ï¼ˆPredicted value of the $ i$-th sampleï¼‰
- $(y_i - \hat{y}_i)^2$ï¼šé¢„æµ‹è¯¯å·®çš„å¹³æ–¹ï¼ˆSquared prediction errorï¼‰
- $\sum_{i=1}^{n}$ï¼šå¯¹æ‰€æœ‰æ ·æœ¬æ±‚å’Œï¼ˆSum over all samplesï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

**ç¬¦å·è§£é‡Šï¼š**
- æœ€å°å€¼ä¸º $0 $ï¼ˆMinimum value is $ 0$ï¼‰ï¼Œå½“æ‰€æœ‰é¢„æµ‹å®Œå…¨æ­£ç¡®æ—¶ï¼ˆWhen all predictions are completely correctï¼‰
- æœ€å¤§å€¼æ— ä¸Šç•Œï¼ˆMaximum value is unboundedï¼‰ï¼Œè¯¯å·®è¶Šå¤§ï¼ŒæŸå¤±è¶Šå¤§ï¼ˆLarger error leads to larger lossï¼‰

**æ¢¯åº¦ï¼š**

$$\frac{\partial L_{\text{MSE}}}{\partial \hat{y}_i} = -2(y_i - \hat{y}_i)$$

**ç¬¦å·è§£é‡Šï¼š**
- $\frac{\partial L_{\text{MSE}}}{\partial \hat{y}_i}$ï¼šæŸå¤±å‡½æ•°å¯¹é¢„æµ‹å€¼çš„æ¢¯åº¦ï¼ˆGradient of loss function with respect to predictionï¼‰
- æ¢¯åº¦ä¸è¯¯å·®æˆæ­£æ¯”ï¼ˆGradient is proportional to errorï¼‰
- å¤§è¯¯å·®äº§ç”Ÿå¤§æ¢¯åº¦ï¼ˆLarge errors produce large gradientsï¼‰

![MSE Loss Function](../../images/loss_functions/mse_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **æ•°å­¦æ€§è´¨å¥½**ï¼šå¤„å¤„å¯å¯¼ï¼Œå‡¸å‡½æ•°ï¼ˆDifferentiable everywhere, convex functionï¼‰
- **å¯¹å¤§è¯¯å·®æ•æ„Ÿ**ï¼šå¹³æ–¹é¡¹ä½¿å¾—å¤§è¯¯å·®è¢«æ”¾å¤§ï¼ˆSquared term amplifies large errorsï¼‰
- **ç”¨é€”**ï¼šå›å½’é—®é¢˜ï¼ˆRegression problemsï¼‰ï¼Œå‡è®¾è¯¯å·®æœä»é«˜æ–¯åˆ†å¸ƒï¼ˆAssumes errors follow Gaussian distributionï¼‰
- **ç¼ºç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆSensitive to outliersï¼‰

##### 1.2 MAEï¼ˆMean Absolute Errorï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{MAE}}$ï¼šå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Errorï¼‰
- $|y_i - \hat{y}_i|$ï¼šé¢„æµ‹è¯¯å·®çš„ç»å¯¹å€¼ï¼ˆAbsolute value of prediction errorï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

**ç¬¦å·è§£é‡Šï¼š**
- æœ€å°å€¼ä¸º $0 $ï¼ˆMinimum value is $ 0$ï¼‰
- æœ€å¤§å€¼æ— ä¸Šç•Œï¼ˆMaximum value is unboundedï¼‰

**æ¢¯åº¦ï¼š**

$$\frac{\partial L_{\text{MAE}}}{\partial \hat{y}_i} = \begin{cases} -1 & \text{if } \hat{y}_i < y_i \\ +1 & \text{if } \hat{y}_i > y_i \\ \text{undefined} & \text{if } \hat{y}_i = y_i \end{cases}$$

**ç¬¦å·è§£é‡Šï¼š**
- æ¢¯åº¦åœ¨è¯¯å·®ä¸º0å¤„ä¸å¯å¯¼ï¼ˆGradient is not differentiable at zero errorï¼‰
- æ¢¯åº¦å¤§å°æ’ä¸º1ï¼ˆGradient magnitude is constant 1ï¼‰ï¼Œä¸å—è¯¯å·®å¤§å°å½±å“ï¼ˆNot affected by error magnitudeï¼‰

![MAE Loss Function](../../images/loss_functions/mae_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **å¯¹å¼‚å¸¸å€¼é²æ£’**ï¼šç»å¯¹å€¼ä½¿å¾—å¼‚å¸¸å€¼çš„å½±å“è¾ƒå°ï¼ˆAbsolute value makes outliers less influentialï¼‰
- **ç”¨é€”**ï¼šå›å½’é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å­˜åœ¨å¼‚å¸¸å€¼æ—¶ï¼ˆRegression problems, especially when outliers existï¼‰
- **ç¼ºç‚¹**ï¼šåœ¨0å¤„ä¸å¯å¯¼ï¼ˆNot differentiable at zeroï¼‰

##### 1.3 Huber Loss

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{Huber}} = \begin{cases} \frac{1}{2}(y_i - \hat{y}_i)^2 & \text{if } |y_i - \hat{y}_i| \leq \delta \\ \delta|y_i - \hat{y}_i| - \frac{1}{2}\delta^2 & \text{if } |y_i - \hat{y}_i| > \delta \end{cases}$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{Huber}}$ï¼šHuberæŸå¤±ï¼ˆHuber Lossï¼‰
- $\delta $ï¼šé˜ˆå€¼å‚æ•°ï¼ˆThreshold parameterï¼‰ï¼Œé€šå¸¸å– $ 1.0 $ï¼ˆTypically $ 1.0$ï¼‰
- å½“è¯¯å·®å°äºç­‰äº $\delta $ æ—¶ï¼Œä½¿ç”¨MSEï¼ˆWhen error $\leq \delta$, use MSEï¼‰
- å½“è¯¯å·®å¤§äº $\delta $ æ—¶ï¼Œä½¿ç”¨MAEï¼ˆWhen error $> \delta$, use MAEï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

![Huber Loss Function](../../images/loss_functions/huber_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **ç»“åˆMSEå’ŒMAE**ï¼šå°è¯¯å·®æ—¶åƒMSEï¼ˆå¹³æ»‘ï¼‰ï¼Œå¤§è¯¯å·®æ—¶åƒMAEï¼ˆé²æ£’ï¼‰ï¼ˆSmall errors like MSE (smooth), large errors like MAE (robust)ï¼‰
- **å¯¹å¼‚å¸¸å€¼é²æ£’**ï¼šå¤§è¯¯å·®æ—¶ä½¿ç”¨çº¿æ€§æƒ©ç½šï¼ˆLinear penalty for large errorsï¼‰
- **ç”¨é€”**ï¼šå›å½’é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯éœ€è¦å¹³è¡¡å¹³æ»‘æ€§å’Œé²æ£’æ€§æ—¶ï¼ˆRegression problems, especially when balancing smoothness and robustnessï¼‰

##### 1.4 Quantile Lossï¼ˆåˆ†ä½æ•°æŸå¤±ï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{Quantile}} = \sum_{i=1}^{n} \begin{cases} \tau(y_i - \hat{y}_i) & \text{if } y_i \geq \hat{y}_i \\ (1-\tau)(\hat{y}_i - y_i) & \text{if } y_i < \hat{y}_i \end{cases}$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{Quantile}}$ï¼šåˆ†ä½æ•°æŸå¤±ï¼ˆQuantile Lossï¼‰
- $\tau $ï¼šåˆ†ä½æ•°å‚æ•°ï¼ˆQuantile parameterï¼‰ï¼Œ$ 0 < \tau < 1$ï¼ˆBetween 0 and 1ï¼‰
- å½“é¢„æµ‹å€¼å°äºçœŸå®å€¼æ—¶ï¼Œä½¿ç”¨æƒé‡ $\tau $ï¼ˆWhen prediction < true value, use weight $\tau$ï¼‰
- å½“é¢„æµ‹å€¼å¤§äºçœŸå®å€¼æ—¶ï¼Œä½¿ç”¨æƒé‡ $1-\tau $ï¼ˆWhen prediction > true value, use weight $ 1-\tau$ï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

![Quantile Loss Function](../../images/loss_functions/quantile_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **åˆ†ä½æ•°å›å½’**ï¼šå¯ä»¥é¢„æµ‹ä¸åŒåˆ†ä½æ•°ï¼ˆCan predict different quantilesï¼‰
- **éå¯¹ç§°æƒ©ç½š**ï¼šå¯¹ä¸åŒæ–¹å‘çš„è¯¯å·®ç»™äºˆä¸åŒæƒé‡ï¼ˆAsymmetric penalty for errors in different directionsï¼‰
- **ç”¨é€”**ï¼šéœ€è¦é¢„æµ‹åˆ†ä½æ•°çš„åœºæ™¯ï¼Œå¦‚é£é™©é¢„æµ‹ï¼ˆScenarios requiring quantile prediction, such as risk predictionï¼‰

#### 2. åˆ†ç±»æŸå¤±å‡½æ•°

##### 2.1 Binary Cross-Entropyï¼ˆäºŒåˆ†ç±»äº¤å‰ç†µï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{BCE}} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{BCE}}$ï¼šäºŒåˆ†ç±»äº¤å‰ç†µï¼ˆBinary Cross-Entropyï¼‰
- $y_i \in \{0, 1\}$ï¼šçœŸå®æ ‡ç­¾ï¼ˆTrue labelï¼‰ï¼Œ0æˆ–1
- $\hat{y}_i \in (0, 1)$ï¼šé¢„æµ‹æ¦‚ç‡ï¼ˆPredicted probabilityï¼‰ï¼Œåœ¨0å’Œ1ä¹‹é—´
- $\log$ï¼šè‡ªç„¶å¯¹æ•°ï¼ˆNatural logarithmï¼‰
- $y_i \log(\hat{y}_i)$ï¼šå½“çœŸå®æ ‡ç­¾ä¸º1æ—¶çš„æŸå¤±é¡¹ï¼ˆLoss term when true label is 1ï¼‰
- $(1-y_i) \log(1-\hat{y}_i)$ï¼šå½“çœŸå®æ ‡ç­¾ä¸º0æ—¶çš„æŸå¤±é¡¹ï¼ˆLoss term when true label is 0ï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

**ç¬¦å·è§£é‡Šï¼š**
- æœ€å°å€¼ä¸º $0 $ï¼ˆMinimum value is $ 0$ï¼‰ï¼Œå½“é¢„æµ‹å®Œå…¨æ­£ç¡®æ—¶ï¼ˆWhen predictions are completely correctï¼‰
- å½“é¢„æµ‹å®Œå…¨é”™è¯¯æ—¶ï¼ŒæŸå¤±è¶‹å‘æ— ç©·ï¼ˆWhen predictions are completely wrong, loss tends to infinityï¼‰

**æ¢¯åº¦ï¼š**

$$\frac{\partial L_{\text{BCE}}}{\partial \hat{y}_i} = -\frac{y_i}{\hat{y}_i} + \frac{1-y_i}{1-\hat{y}_i} = \frac{\hat{y}_i - y_i}{\hat{y}_i(1-\hat{y}_i)}$$

**ç¬¦å·è§£é‡Šï¼š**
- æ¢¯åº¦ä¸é¢„æµ‹è¯¯å·®æˆæ­£æ¯”ï¼ˆGradient is proportional to prediction errorï¼‰
- å½“é¢„æµ‹æ¥è¿‘0æˆ–1æ—¶ï¼Œæ¢¯åº¦å¯èƒ½å¾ˆå¤§ï¼ˆWhen prediction is close to 0 or 1, gradient may be largeï¼‰

![Binary Cross-Entropy Loss Function](../../images/loss_functions/binary_cross_entropy_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **æ¦‚ç‡è§£é‡Š**ï¼šé€‚ç”¨äºæ¦‚ç‡è¾“å‡ºï¼ˆSuitable for probability outputï¼‰
- **ç”¨é€”**ï¼šäºŒåˆ†ç±»é—®é¢˜ï¼ˆBinary classification problemsï¼‰ï¼Œé€šå¸¸é…åˆSigmoidæ¿€æ´»å‡½æ•°ï¼ˆUsually with Sigmoid activationï¼‰

##### 2.2 Multi-Class Cross-Entropyï¼ˆå¤šåˆ†ç±»äº¤å‰ç†µï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{CCE}} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{C} y_{ic} \log(\hat{y}_{ic})$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{CCE}}$ï¼šå¤šåˆ†ç±»äº¤å‰ç†µï¼ˆMulti-Class Cross-Entropyï¼‰ï¼Œä¹Ÿå«Categorical Cross-Entropy
- $C$ï¼šç±»åˆ«æ•°é‡ï¼ˆNumber of classesï¼‰
- $y_{ic}$ï¼šç¬¬ $ i $ ä¸ªæ ·æœ¬å±äºç±»åˆ« $ c $ çš„çœŸå®æ ‡ç­¾ï¼ˆTrue label of the $ i $-th sample for class $ c$ï¼‰ï¼Œone-hotç¼–ç ï¼ˆOne-hot encodingï¼‰
- $\hat{y}_{ic}$ï¼šç¬¬ $ i $ ä¸ªæ ·æœ¬å±äºç±»åˆ« $ c $ çš„é¢„æµ‹æ¦‚ç‡ï¼ˆPredicted probability of the $ i $-th sample for class $ c$ï¼‰
- $\sum_{c=1}^{C}$ï¼šå¯¹æ‰€æœ‰ç±»åˆ«æ±‚å’Œï¼ˆSum over all classesï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

![Multi-Class Cross-Entropy Loss Function](../../images/loss_functions/multi_class_cross_entropy_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **å¤šåˆ†ç±»é—®é¢˜**ï¼šé€‚ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼ˆSuitable for multi-class classificationï¼‰
- **ç”¨é€”**ï¼šå¤šåˆ†ç±»é—®é¢˜ï¼ˆMulti-class classification problemsï¼‰ï¼Œé€šå¸¸é…åˆSoftmaxæ¿€æ´»å‡½æ•°ï¼ˆUsually with Softmax activationï¼‰

##### 2.3 Focal Loss

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{Focal}} = -\alpha_t (1-p_t)^{\gamma} \log(p_t)$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{Focal}}$ï¼šFocalæŸå¤±ï¼ˆFocal Lossï¼‰
- $\alpha_t$ï¼šç±»åˆ«æƒé‡ï¼ˆClass weightï¼‰ï¼Œå¹³è¡¡ç±»åˆ«ä¸å¹³è¡¡ï¼ˆBalances class imbalanceï¼‰
- $p_t$ï¼šé¢„æµ‹æ¦‚ç‡ï¼ˆPredicted probabilityï¼‰ï¼Œå¯¹äºæ­£ç¡®ç±»åˆ«ï¼ˆFor correct classï¼‰
- $\gamma $ï¼šèšç„¦å‚æ•°ï¼ˆFocusing parameterï¼‰ï¼Œé€šå¸¸å– $ 2 $ï¼ˆTypically $ 2$ï¼‰
- $(1-p_t)^{\gamma}$ï¼šè°ƒåˆ¶å› å­ï¼ˆModulating factorï¼‰ï¼Œé™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼ˆReduces weight of easy examplesï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

![Focal Loss Function](../../images/loss_functions/focal_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **å¤„ç†ç±»åˆ«ä¸å¹³è¡¡**ï¼šé€šè¿‡ $\alpha_t $ å¹³è¡¡ç±»åˆ«ï¼ˆBalances classes through $\alpha_t$ï¼‰
- **èšç„¦éš¾æ ·æœ¬**ï¼šé€šè¿‡ $(1-p_t)^{\gamma}$ èšç„¦éš¾åˆ†ç±»æ ·æœ¬ï¼ˆFocuses on hard examples through $(1-p_t)^{\gamma}$ï¼‰
- **ç”¨é€”**ï¼šç›®æ ‡æ£€æµ‹ï¼ˆObject detectionï¼‰ï¼Œç‰¹åˆ«æ˜¯ç±»åˆ«ä¸å¹³è¡¡çš„åœºæ™¯ï¼ˆEspecially in class-imbalanced scenariosï¼‰

##### 2.4 Hinge Lossï¼ˆåˆé¡µæŸå¤±ï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{Hinge}} = \max(0, 1 - y_i \hat{y}_i)$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{Hinge}}$ï¼šHingeæŸå¤±ï¼ˆHinge Lossï¼‰
- $y_i \in \{-1, +1\}$ï¼šçœŸå®æ ‡ç­¾ï¼ˆTrue labelï¼‰ï¼Œ-1æˆ–+1
- $\hat{y}_i$ï¼šé¢„æµ‹å€¼ï¼ˆPredicted valueï¼‰ï¼Œå¯ä»¥æ˜¯ä»»æ„å®æ•°ï¼ˆCan be any real numberï¼‰
- $y_i \hat{y}_i $ï¼šå½“ $ y_i $ å’Œ $\hat{y}_i $ åŒå·ä¸”ä¹˜ç§¯å¤§äº1æ—¶ï¼ŒæŸå¤±ä¸º0ï¼ˆWhen $ y_i $ and $\hat{y}_i$ have same sign and product > 1, loss is 0ï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

![Hinge Loss Function](../../images/loss_functions/hinge_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **æœ€å¤§é—´éš”**ï¼šé¼“åŠ±åˆ†ç±»é—´éš”æœ€å¤§åŒ–ï¼ˆEncourages maximum classification marginï¼‰
- **ç”¨é€”**ï¼šæ”¯æŒå‘é‡æœºï¼ˆSVMï¼ŒSupport Vector Machinesï¼‰ï¼ŒäºŒåˆ†ç±»é—®é¢˜ï¼ˆBinary classification problemsï¼‰

#### 3. ç‰¹æ®ŠæŸå¤±å‡½æ•°

##### 3.1 KLæ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$D_{\text{KL}}(P||Q) = \sum_{i} P(i) \log\frac{P(i)}{Q(i)}$$

**ç¬¦å·è§£é‡Šï¼š**
- $D_{\text{KL}}(P||Q)$ï¼šKLæ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰ï¼Œè¡¡é‡åˆ†å¸ƒ $ P $ å’Œ $ Q $ çš„å·®å¼‚ï¼ˆMeasures difference between distributions $ P $ and $ Q$ï¼‰
- $P(i)$ï¼šåˆ†å¸ƒ $ P $ åœ¨ $ i $ å¤„çš„æ¦‚ç‡ï¼ˆProbability of distribution $ P $ at $ i$ï¼‰
- $Q(i)$ï¼šåˆ†å¸ƒ $ Q $ åœ¨ $ i $ å¤„çš„æ¦‚ç‡ï¼ˆProbability of distribution $ Q $ at $ i$ï¼‰
- $\log\frac{P(i)}{Q(i)}$ï¼šå¯¹æ•°æ¯”ç‡ï¼ˆLog ratioï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

**ç¬¦å·è§£é‡Šï¼š**
- æœ€å°å€¼ä¸º $0 $ï¼ˆMinimum value is $ 0 $ï¼‰ï¼Œå½“ $ P = Q $ æ—¶ï¼ˆWhen $ P = Q$ï¼‰
- éå¯¹ç§°ï¼ˆAsymmetricï¼‰ï¼š$D_{\text{KL}}(P||Q) \neq D_{\text{KL}}(Q||P)$

![KL Divergence Loss Function](../../images/loss_functions/kl_divergence_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **åˆ†å¸ƒè·ç¦»**ï¼šè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼ˆMeasures difference between two probability distributionsï¼‰
- **ç”¨é€”**ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼ŒVariational Autoencodersï¼‰ï¼ŒçŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰ï¼Œæ¨¡å‹æ­£åˆ™åŒ–ï¼ˆModel regularizationï¼‰

##### 3.2 Contrastive Lossï¼ˆå¯¹æ¯”æŸå¤±ï¼‰

**æ•°å­¦å…¬å¼ï¼š**

$$L_{\text{Contrastive}} = \frac{1}{2n} \sum_{i=1}^{n} [y_i d_i^2 + (1-y_i) \max(0, m-d_i)^2]$$

**ç¬¦å·è§£é‡Šï¼š**
- $L_{\text{Contrastive}}$ï¼šå¯¹æ¯”æŸå¤±ï¼ˆContrastive Lossï¼‰
- $y_i \in \{0, 1\}$ï¼šæ ·æœ¬å¯¹æ˜¯å¦ç›¸ä¼¼ï¼ˆWhether sample pair is similarï¼‰ï¼Œ1è¡¨ç¤ºç›¸ä¼¼ï¼Œ0è¡¨ç¤ºä¸ç›¸ä¼¼
- $d_i$ï¼šæ ·æœ¬å¯¹ä¹‹é—´çš„è·ç¦»ï¼ˆDistance between sample pairï¼‰ï¼Œé€šå¸¸æ˜¯æ¬§æ°è·ç¦»ï¼ˆUsually Euclidean distanceï¼‰
- $m$ï¼šè¾¹ç•Œå‚æ•°ï¼ˆMargin parameterï¼‰ï¼Œä¸ç›¸ä¼¼æ ·æœ¬å¯¹çš„æœ€å°è·ç¦»ï¼ˆMinimum distance for dissimilar pairsï¼‰

**è¾“å‡ºèŒƒå›´ï¼š**

$$[0, +\infty)$$

![Contrastive Loss Function](../../images/loss_functions/contrastive_loss.png)

**å¥½å¤„å’Œç”¨é€”ï¼š**
- **å­¦ä¹ è¡¨ç¤º**ï¼šå­¦ä¹ ä½¿å¾—ç›¸ä¼¼æ ·æœ¬æ¥è¿‘ã€ä¸ç›¸ä¼¼æ ·æœ¬è¿œç¦»çš„è¡¨ç¤ºï¼ˆLearns representations that bring similar samples close and dissimilar samples farï¼‰
- **ç”¨é€”**ï¼šåº¦é‡å­¦ä¹ ï¼ˆMetric Learningï¼‰ï¼Œäººè„¸è¯†åˆ«ï¼ˆFace Recognitionï¼‰ï¼Œç›¸ä¼¼åº¦å­¦ä¹ ï¼ˆSimilarity Learningï¼‰

---

### äºŒã€å£è¯­åŒ–ç­”æ¡ˆ

#### ä¸­æ–‡ç‰ˆæœ¬

æŸå¤±å‡½æ•°è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼çš„å·®å¼‚ï¼Œæ˜¯æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡ã€‚

å›å½’æŸå¤±åŒ…æ‹¬MSEã€MAEã€Huber Lossã€Quantile Lossã€‚MSEæ˜¯å‡æ–¹è¯¯å·®ï¼Œè¾“å‡ºèŒƒå›´0åˆ°æ­£æ— ç©·ï¼Œå¯¹å¤§è¯¯å·®æ•æ„Ÿï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼Œé€‚ç”¨äºå›å½’é—®é¢˜ã€‚MAEæ˜¯å¹³å‡ç»å¯¹è¯¯å·®ï¼Œè¾“å‡ºèŒƒå›´0åˆ°æ­£æ— ç©·ï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’ï¼Œä½†åœ¨0å¤„ä¸å¯å¯¼ã€‚Huber Lossç»“åˆMSEå’ŒMAEï¼Œå°è¯¯å·®æ—¶åƒMSEï¼Œå¤§è¯¯å·®æ—¶åƒMAEï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’ã€‚Quantile Lossæ˜¯åˆ†ä½æ•°æŸå¤±ï¼Œå¯ä»¥é¢„æµ‹ä¸åŒåˆ†ä½æ•°ï¼Œéå¯¹ç§°æƒ©ç½šï¼Œé€‚ç”¨äºéœ€è¦é¢„æµ‹åˆ†ä½æ•°çš„åœºæ™¯ã€‚

åˆ†ç±»æŸå¤±åŒ…æ‹¬Binary Cross-Entropyã€Multi-Class Cross-Entropyã€Focal Lossã€Hinge Lossã€‚Binary Cross-Entropyæ˜¯äºŒåˆ†ç±»äº¤å‰ç†µï¼Œè¾“å‡ºèŒƒå›´0åˆ°æ­£æ— ç©·ï¼Œé€‚ç”¨äºæ¦‚ç‡è¾“å‡ºï¼Œé€šå¸¸é…åˆSigmoidã€‚Multi-Class Cross-Entropyæ˜¯å¤šåˆ†ç±»äº¤å‰ç†µï¼Œé€‚ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ï¼Œé€šå¸¸é…åˆSoftmaxã€‚Focal Lossé€šè¿‡è°ƒåˆ¶å› å­èšç„¦éš¾æ ·æœ¬ï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œé€‚ç”¨äºç›®æ ‡æ£€æµ‹ã€‚Hinge Lossé¼“åŠ±æœ€å¤§é—´éš”ï¼Œé€‚ç”¨äºSVMã€‚

ç‰¹æ®ŠæŸå¤±åŒ…æ‹¬KLæ•£åº¦å’ŒContrastive Lossã€‚KLæ•£åº¦è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼Œéå¯¹ç§°ï¼Œé€‚ç”¨äºVAEã€çŸ¥è¯†è’¸é¦ã€‚Contrastive Losså­¦ä¹ ä½¿å¾—ç›¸ä¼¼æ ·æœ¬æ¥è¿‘ã€ä¸ç›¸ä¼¼æ ·æœ¬è¿œç¦»çš„è¡¨ç¤ºï¼Œé€‚ç”¨äºåº¦é‡å­¦ä¹ ã€äººè„¸è¯†åˆ«ã€‚

#### English Version

Loss functions measure the difference between model predictions and true values, serving as optimization objectives.

Regression losses include MSE, MAE, Huber Loss, Quantile Loss. MSE is mean squared error, output range 0 to positive infinity, sensitive to large errors and outliers, suitable for regression. MAE is mean absolute error, output range 0 to positive infinity, robust to outliers but not differentiable at zero. Huber Loss combines MSE and MAE, like MSE for small errors, like MAE for large errors, robust to outliers. Quantile Loss can predict different quantiles, asymmetric penalty, suitable for scenarios requiring quantile prediction.

Classification losses include Binary Cross-Entropy, Multi-Class Cross-Entropy, Focal Loss, Hinge Loss. Binary Cross-Entropy is for binary classification, output range 0 to positive infinity, suitable for probability output, usually with Sigmoid. Multi-Class Cross-Entropy is for multi-class tasks, usually with Softmax. Focal Loss focuses on hard examples through modulating factor, handles class imbalance, suitable for object detection. Hinge Loss encourages maximum margin, suitable for SVM.

Special losses include KL Divergence and Contrastive Loss. KL Divergence measures difference between two probability distributions, asymmetric, suitable for VAE, knowledge distillation. Contrastive Loss learns representations that bring similar samples close and dissimilar samples far, suitable for metric learning, face recognition.

---

### ä¸‰ã€æŸå¤±å‡½æ•°å¯¹æ¯”è¡¨æ ¼

| æŸå¤±å‡½æ•° | å…¬å¼ | è¾“å‡ºèŒƒå›´ | æ¢¯åº¦ç‰¹æ€§ | ä¸»è¦ä¼˜ç‚¹ | ä¸»è¦ç¼ºç‚¹ | å…¸å‹ç”¨é€” | é…åˆæ¿€æ´»å‡½æ•° |
|---------|------|---------|---------|---------|---------|---------|------------|
| **MSE** | $\frac{1}{n}\sum(y_i-\hat{y}_i)^2 $ | $[0, +\infty)$ | ä¸è¯¯å·®æˆæ­£æ¯” | æ•°å­¦æ€§è´¨å¥½ï¼Œå‡¸å‡½æ•° | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ | å›å½’é—®é¢˜ | æ— ï¼ˆç›´æ¥è¾“å‡ºï¼‰ |
| **MAE** | $\frac{1}{n}\sum\|y_i-\hat{y}_i\|$ | $[0, +\infty)$ | æ’ä¸ºÂ±1 | å¯¹å¼‚å¸¸å€¼é²æ£’ | åœ¨0å¤„ä¸å¯å¯¼ | å›å½’ï¼ˆæœ‰å¼‚å¸¸å€¼ï¼‰ | æ— ï¼ˆç›´æ¥è¾“å‡ºï¼‰ |
| **Huber Loss** | MSE if $\|e\|\leq\delta $, else MAE | $[0, +\infty)$ | å°è¯¯å·®å¹³æ»‘ï¼Œå¤§è¯¯å·®çº¿æ€§ | ç»“åˆMSEå’ŒMAEä¼˜ç‚¹ | éœ€è¦è°ƒå‚$\delta$ | å›å½’ï¼ˆå¹³è¡¡å¹³æ»‘å’Œé²æ£’ï¼‰ | æ— ï¼ˆç›´æ¥è¾“å‡ºï¼‰ |
| **Quantile Loss** | $\tau\|e\|$ if $ e\geq0 $, else $(1-\tau)\|e\|$ | $[0, +\infty)$ | éå¯¹ç§°æ¢¯åº¦ | å¯é¢„æµ‹åˆ†ä½æ•° | éœ€è¦é€‰æ‹©$\tau$ | åˆ†ä½æ•°å›å½’ | æ— ï¼ˆç›´æ¥è¾“å‡ºï¼‰ |
| **Binary Cross-Entropy** | $-\frac{1}{n}\sum[y_i\log\hat{y}_i+(1-y_i)\log(1-\hat{y}_i)]$ | $[0, +\infty)$ | $\frac{\hat{y}_i-y_i}{\hat{y}_i(1-\hat{y}_i)}$ | æ¦‚ç‡è§£é‡Šï¼Œé€‚åˆäºŒåˆ†ç±» | é¢„æµ‹æ¥è¿‘0/1æ—¶æ¢¯åº¦å¤§ | äºŒåˆ†ç±»é—®é¢˜ | **Sigmoid** |
| **Multi-Class Cross-Entropy** | $-\frac{1}{n}\sum_i\sum_c y_{ic}\log\hat{y}_{ic}$ | $[0, +\infty)$ | ä¸é¢„æµ‹è¯¯å·®ç›¸å…³ | å¤šåˆ†ç±»ï¼Œæ¦‚ç‡è§£é‡Š | ç±»åˆ«å¤šæ—¶è®¡ç®—é‡å¤§ | å¤šåˆ†ç±»é—®é¢˜ | **Softmax** |
| **Focal Loss** | $-\alpha_t(1-p_t)^{\gamma}\log(p_t)$ | $[0, +\infty)$ | èšç„¦éš¾æ ·æœ¬ | å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œèšç„¦éš¾æ ·æœ¬ | éœ€è¦è°ƒå‚$\alpha,\gamma$ | ç›®æ ‡æ£€æµ‹ | Sigmoid/Softmax |
| **Hinge Loss** | $\max(0, 1-y_i\hat{y}_i)$ | $[0, +\infty)$ | åœ¨è¾¹ç•Œå¤„ä¸è¿ç»­ | æœ€å¤§é—´éš”ï¼Œç¨€ç–æ€§ | ä¸å¯å¯¼åœ¨è¾¹ç•Œ | SVM | æ— ï¼ˆç›´æ¥è¾“å‡ºï¼‰ |
| **KL Divergence** | $\sum_i P(i)\log\frac{P(i)}{Q(i)}$ | $[0, +\infty)$ | éå¯¹ç§° | è¡¡é‡åˆ†å¸ƒå·®å¼‚ | éå¯¹ç§°ï¼Œéè·ç¦» | VAEã€çŸ¥è¯†è’¸é¦ | Softmax |
| **Contrastive Loss** | $y_id_i^2+(1-y_i)\max(0,m-d_i)^2 $ | $[0, +\infty)$ | ä¸è·ç¦»ç›¸å…³ | å­¦ä¹ è¡¨ç¤ºï¼Œç›¸ä¼¼åº¦å­¦ä¹  | éœ€è¦é€‰æ‹©$ m$ | åº¦é‡å­¦ä¹ ã€äººè„¸è¯†åˆ« | æ— ï¼ˆç›´æ¥è¾“å‡ºï¼‰ |

**è®°å¿†è¦ç‚¹ï¼š**
- **å›å½’æŸå¤±**ï¼šMSEå¸¸ç”¨ä½†æ•æ„Ÿï¼ŒMAEé²æ£’ä½†ä¸å¯å¯¼ï¼ŒHuberå¹³è¡¡ä¸¤è€…ï¼ŒQuantileç”¨äºåˆ†ä½æ•°
- **åˆ†ç±»æŸå¤±**ï¼šBCEé…Sigmoidç”¨äºäºŒåˆ†ç±»ï¼ŒCCEé…Softmaxç”¨äºå¤šåˆ†ç±»ï¼ŒFocalå¤„ç†ä¸å¹³è¡¡ï¼ŒHingeç”¨äºSVM
- **ç‰¹æ®ŠæŸå¤±**ï¼šKLç”¨äºåˆ†å¸ƒè·ç¦»ï¼ŒContrastiveç”¨äºè¡¨ç¤ºå­¦ä¹ 
- **é€‰æ‹©åŸåˆ™**ï¼šå›å½’ç”¨MSE/MAEï¼ŒäºŒåˆ†ç±»ç”¨BCEï¼Œå¤šåˆ†ç±»ç”¨CCEï¼Œä¸å¹³è¡¡ç”¨Focalï¼ŒSVMç”¨Hinge

---

## ğŸ“š å¯¼èˆª

â† [ä¸Šä¸€é¢˜: å¸¸ç”¨æ¿€æ´»å‡½æ•°](activation_functions.md)

â†’ [ä¸‹ä¸€é¢˜: æ­£åˆ™åŒ–åŸºç¡€çŸ¥è¯†](../regularization/0_basic_concepts.md)