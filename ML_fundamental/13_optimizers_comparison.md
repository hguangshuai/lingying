# ä¼˜åŒ–å™¨å¯¹æ¯”ï¼ˆOptimizers Comparisonï¼‰

## é—®é¢˜ï¼šä¸åŒä¼˜åŒ–å™¨ï¼ˆSGDã€Momentumã€Adagradã€RMSpropã€Adamï¼‰çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

### ä¸€ã€è¯¦ç»†ä¸­æ–‡è§£é‡Š

ä¼˜åŒ–å™¨ï¼ˆOptimizersï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­è‡³å…³é‡è¦çš„ç»„ä»¶ï¼Œå†³å®šäº†å‚æ•°æ›´æ–°çš„æ–¹å¼å’Œæ•ˆç‡ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨åœ¨æ”¶æ•›é€Ÿåº¦ã€ç¨³å®šæ€§ã€å†…å­˜ä½¿ç”¨å’Œè¶…å‚æ•°æ•æ„Ÿåº¦ç­‰æ–¹é¢æœ‰æ˜¾è‘—å·®å¼‚ã€‚ç†è§£æ¯ç§ä¼˜åŒ–å™¨çš„æœºåˆ¶æœ‰åŠ©äºé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥ã€‚

#### 1. SGDï¼ˆStochastic Gradient Descentï¼‰

$$\theta_{t+1} = \theta_t - \alpha \cdot \nabla L(\theta_t)$$

**ç¬¦å·è§£é‡Šï¼š** $\alpha$ =å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰ï¼›$\nabla L(\theta_t)$ =åœ¨å‚æ•° $\theta_t$ å¤„çš„æ¢¯åº¦ã€‚

**æ ¸å¿ƒæœºåˆ¶ï¼š** æœ€ç®€å•çš„å‚æ•°æ›´æ–°æ–¹å¼ï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°ã€‚

**è¯¦ç»†åˆ†æï¼š**
- **ä¼˜ç‚¹**ï¼š
  - ç®—æ³•ç®€å•ï¼Œæ˜“äºç†è§£å’Œå®ç°
  - å†…å­˜æ•ˆç‡é«˜ï¼Œåªéœ€è¦å­˜å‚¨å½“å‰æ¢¯åº¦
  - åœ¨å‡¸ä¼˜åŒ–é—®é¢˜ä¸Šä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜
  - æ— é¢å¤–è¶…å‚æ•°ï¼ˆé™¤å­¦ä¹ ç‡å¤–ï¼‰

- **ç¼ºç‚¹**ï¼š
  - æ”¶æ•›é€Ÿåº¦æ…¢ï¼Œå°¤å…¶åœ¨å³¡è°·åœ°å½¢ï¼ˆvalley landscapesï¼‰ä¸­å®¹æ˜“æŒ¯è¡
  - å¯¹å­¦ä¹ ç‡éå¸¸æ•æ„Ÿï¼šè¿‡å°æ”¶æ•›æ…¢ï¼Œè¿‡å¤§ä¼šå‘æ•£
  - å¯èƒ½åœ¨å±€éƒ¨æœ€å°å€¼é™„è¿‘æŒ¯è¡ï¼Œæ— æ³•ç¨³å®šæ”¶æ•›
  - æ¢¯åº¦å™ªå£°å¤§ï¼Œå¯¼è‡´æ›´æ–°æ–¹å‘ä¸ç¨³å®š

- **é€‚ç”¨åœºæ™¯**ï¼šå°æ•°æ®é›†ã€å‡¸ä¼˜åŒ–é—®é¢˜ã€éœ€è¦ç²¾ç¡®æ§åˆ¶æ›´æ–°è¿‡ç¨‹çš„åœºæ™¯

![SGD Optimizer Visualization](/images/deep_learning/sgd_detailed.png)

#### 2. Momentum

$$v_t = \beta \cdot v_{t-1} + \nabla L(\theta_t)$$

$$\theta_{t+1} = \theta_t - \alpha \cdot v_t$$

**ç¬¦å·è§£é‡Šï¼š** $v_t$ =é€Ÿåº¦é¡¹ï¼ˆVelocity Termï¼‰ï¼Œç§¯ç´¯äº†å†å²æ¢¯åº¦ä¿¡æ¯ï¼›$\beta$ =åŠ¨é‡ç³»æ•°ï¼ˆMomentum Coefficientï¼‰ï¼Œé€šå¸¸è®¾ä¸º 0.9ã€‚

**æ ¸å¿ƒæœºåˆ¶ï¼š** å¼•å…¥é€Ÿåº¦æ¦‚å¿µï¼Œå½“å‰æ›´æ–°ä¸ä»…è€ƒè™‘å½“å‰æ¢¯åº¦ï¼Œè¿˜è€ƒè™‘å†å²æ¢¯åº¦çš„ç´¯ç§¯æ•ˆåº”ã€‚

**è¯¦ç»†åˆ†æï¼š**
- **ä¼˜ç‚¹**ï¼š
  - åŠ é€Ÿæ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰ steady gradient çš„æ–¹å‘ä¸Š
  - å‡å°‘æŒ¯è¡ï¼šåœ¨å³¡è°·åœ°å½¢ä¸­èƒ½æ›´å¿«åˆ°è¾¾è°·åº•
  - æœ‰åŠ©äºè·³å‡ºæµ…å±‚å±€éƒ¨æœ€å°å€¼ï¼ˆshallow local minimaï¼‰
  - å¹³æ»‘æ¢¯åº¦å™ªå£°ï¼Œæä¾›æ›´ç¨³å®šçš„æ›´æ–°æ–¹å‘

- **ç¼ºç‚¹**ï¼š
  - å¼•å…¥é¢å¤–è¶…å‚æ•° $\beta$ éœ€è¦è°ƒå‚
  - åœ¨æ¢¯åº¦æ–¹å‘é¢‘ç¹æ”¹å˜æ—¶å¯èƒ½ç§¯ç´¯è¿‡å¤šå†å²ä¿¡æ¯
  - å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹è¿‡å†²ï¼ˆovershootï¼‰æœ€ä¼˜è§£

- **é€‚ç”¨åœºæ™¯**ï¼šéå‡¸ä¼˜åŒ–ã€å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè®­ç»ƒã€å­˜åœ¨å™ªå£°æ¢¯åº¦çš„é—®é¢˜

![Momentum Optimizer Visualization](/images/deep_learning/momentum_detailed.png)

#### 3. Adagradï¼ˆAdaptive Gradientï¼‰

$$G_t = G_{t-1} + g_t^2$$

$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}} \cdot g_t$$

**ç¬¦å·è§£é‡Šï¼š** $G_t$ =ç´¯ç§¯å¹³æ–¹æ¢¯åº¦çŸ©é˜µï¼Œå¯¹è§’çŸ©é˜µå­˜å‚¨æ¯ä¸ªå‚æ•°çš„å†å²å¹³æ–¹æ¢¯åº¦ï¼›$\epsilon$ =å°å¸¸æ•°é˜²æ­¢é™¤é›¶ï¼Œé€šå¸¸ $10^{-8}$ã€‚

**æ ¸å¿ƒæœºåˆ¶ï¼š** ä¸ºæ¯ä¸ªå‚æ•°å•ç‹¬è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ ¹æ®è¯¥å‚æ•°çš„å†å²æ¢¯åº¦å¤§å°è‡ªé€‚åº”ç¼©æ”¾ã€‚

**è¯¦ç»†åˆ†æï¼š**
- **ä¼˜ç‚¹**ï¼š
  - è‡ªé€‚åº”å­¦ä¹ ç‡ï¼šé¢‘ç¹æ›´æ–°çš„å‚æ•°è·å¾—æ›´å°çš„å­¦ä¹ ç‡ï¼Œåä¹‹äº¦ç„¶
  - ç‰¹åˆ«é€‚åˆç¨€ç–æ•°æ®å’Œç‰¹å¾ï¼ˆSparse Featuresï¼‰
  - æ— éœ€æ‰‹åŠ¨è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡
  - åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°è‰¯å¥½

- **ç¼ºç‚¹**ï¼š
  - å­¦ä¹ ç‡å•è°ƒé€’å‡ï¼š$G_t$ ä¸æ–­ç´¯ç§¯ï¼Œå¯¼è‡´å­¦ä¹ ç‡è¶‹è¿‘äº 0
  - åœ¨è®­ç»ƒåæœŸå­¦ä¹ ç‡è¿‡å°ï¼Œæ”¶æ•›åœæ­¢
  - å¯¹å‚æ•°åˆå§‹åŒ–æ•æ„Ÿ
  - å†…å­˜å ç”¨è¾ƒå¤§ï¼ˆéœ€è¦å­˜å‚¨å®Œæ•´çš„ $G_t$ çŸ©é˜µï¼‰

- **é€‚ç”¨åœºæ™¯**ï¼šç¨€ç–æ•°æ®ã€NLPä»»åŠ¡ã€ç‰¹å¾é‡è¦åº¦å·®å¼‚å¤§çš„é—®é¢˜

![Adagrad Optimizer Visualization](/images/deep_learning/adagrad_detailed.png)

#### 4. RMSpropï¼ˆRoot Mean Square Propagationï¼‰

$$E[g^2]_t = \rho \cdot E[g^2]_{t-1} + (1-\rho) \cdot g_t^2$$

$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t$$

**ç¬¦å·è§£é‡Šï¼š** $E[g^2]_t$ =å¹³æ–¹æ¢¯åº¦çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼›$\rho$ =è¡°å‡ç‡ï¼ˆDecay Rateï¼‰ï¼Œé€šå¸¸ 0.9ã€‚

**æ ¸å¿ƒæœºåˆ¶ï¼š** ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›¿ä»£ç´¯ç§¯å’Œï¼Œè§£å†³ Adagrad å­¦ä¹ ç‡é€’å‡è¿‡å¿«çš„é—®é¢˜ã€‚

**è¯¦ç»†åˆ†æï¼š**
- **ä¼˜ç‚¹**ï¼š
  - ä¿æŒ Adagrad çš„è‡ªé€‚åº”ä¼˜åŠ¿
  - è§£å†³å­¦ä¹ ç‡å•è°ƒé€’å‡é—®é¢˜ï¼Œæä¾›ç¨³å®šçš„æ”¶æ•›
  - å¯¹ä¸åŒç±»å‹çš„æ¢¯åº¦åˆ†å¸ƒæ›´é²æ£’
  - è®­ç»ƒè¿‡ç¨‹æ›´ç¨³å®šï¼Œä¸æ˜“å‘æ•£

- **ç¼ºç‚¹**ï¼š
  - ä»éœ€è¦è°ƒå‚ $\rho$ å‚æ•°
  - ç›¸æ¯” SGD/Momentum æœ‰æ›´é«˜çš„è®¡ç®—å¤æ‚åº¦
  - åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ”¶æ•›åˆ°æ¬¡ä¼˜è§£

- **é€‚ç”¨åœºæ™¯**ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€åœ¨çº¿å­¦ä¹ ã€éå¹³ç¨³ç›®æ ‡å‡½æ•°

![RMSprop Optimizer Visualization](/images/deep_learning/rmsprop_detailed.png)

#### 5. Adamï¼ˆAdaptive Moment Estimationï¼‰

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$

$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$

$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$$

$$\theta_{t+1} = \theta_t - \frac{\alpha \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

**ç¬¦å·è§£é‡Šï¼š** $m_t$ =ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡ï¼‰ï¼›$v_t$ =äºŒé˜¶çŸ©ä¼°è®¡ï¼ˆå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡ï¼‰ï¼›$\hat{m}_t, \hat{v}_t$ =åå·®æ ¡æ­£åçš„çŸ©ä¼°è®¡ã€‚

**æ ¸å¿ƒæœºåˆ¶ï¼š** ç»“åˆ Momentum å’Œ RMSpropï¼ŒåŒæ—¶ä¼°è®¡æ¢¯åº¦çš„ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ï¼Œå¹¶è¿›è¡Œåå·®æ ¡æ­£ã€‚

**è¯¦ç»†åˆ†æï¼š**
- **ä¼˜ç‚¹**ï¼š
  - è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ŒåŒæ—¶è€ƒè™‘æ¢¯åº¦çš„å‡å€¼å’Œæ–¹å·®
  - åå·®æ ¡æ­£è§£å†³åˆå§‹åŒ–åå·®é—®é¢˜
  - è®¡ç®—é«˜æ•ˆï¼Œå†…å­˜ä½¿ç”¨åˆç†
  - å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿï¼Œé»˜è®¤è®¾ç½®é€šå¸¸å·¥ä½œè‰¯å¥½
  - åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°ç¨³å¥

- **ç¼ºç‚¹**ï¼š
  - ç†è®ºä¸Šä¸ä¿è¯æ”¶æ•›ï¼ˆä½†å®è·µä¸­è¡¨ç°è‰¯å¥½ï¼‰
  - åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ³›åŒ–èƒ½åŠ›ç¨å¼±
  - ä¸‰ä¸ªè¶…å‚æ•°éœ€è¦ç†è§£ï¼ˆè™½ç„¶é»˜è®¤å€¼é€šå¸¸è¶³å¤Ÿï¼‰

- **é€‚ç”¨åœºæ™¯**ï¼šå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡çš„é»˜è®¤é€‰æ‹©ï¼Œç‰¹åˆ«é€‚åˆå¤§è§„æ¨¡ç½‘ç»œå’Œå¤æ‚ä¼˜åŒ–é—®é¢˜

![Adam Optimizer Visualization](/images/deep_learning/adam_detailed.png)

#### 6. ä¼˜åŒ–å™¨å¯¹æ¯”æ€»ç»“

ä¸ºäº†æ›´ç›´è§‚åœ°ç†è§£ä¸åŒä¼˜åŒ–å™¨çš„ç‰¹ç‚¹ï¼Œä»¥ä¸‹æ˜¯å®ƒä»¬çš„ç»¼åˆå¯¹æ¯”ï¼š

![Optimizers Comparison](/images/deep_learning/optimizers_comparison.png)

**æ€§èƒ½å¯¹æ¯”åˆ†æï¼š**
- **æ”¶æ•›é€Ÿåº¦**ï¼šAdam > RMSprop > Momentum > Adagrad > SGD
- **ç¨³å®šæ€§**ï¼šRMSprop/Adam > Adagrad > Momentum > SGD
- **å†…å­˜æ•ˆç‡**ï¼šSGD/Momentum > Adam > RMSprop > Adagrad
- **è°ƒå‚å¤æ‚åº¦**ï¼šAdam â‰ˆ RMSprop > Momentum > Adagrad > SGD

#### 7. æ¨èä½¿ç”¨æŒ‡å—ï¼ˆRecommendationï¼‰

**é»˜è®¤é€‰æ‹©**ï¼š
- **Adam**ï¼šå¤§å¤šæ•°æƒ…å†µä¸‹é¦–é€‰ï¼Œå¹³è¡¡äº†æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§
- **å­¦ä¹ ç‡**ï¼šAdam é»˜è®¤ 0.001ï¼Œé€šå¸¸ä¸éœ€è¦è°ƒæ•´
- **å…¶ä»–å‚æ•°**ï¼šÎ²â‚=0.9, Î²â‚‚=0.999, Îµ=10â»â¸

**ç‰¹æ®Šåœºæ™¯é€‰æ‹©**ï¼š
- **è®¡ç®—èµ„æºæœ‰é™**ï¼šä½¿ç”¨ SGD + Momentum
- **ç¨€ç–æ•°æ®/ç‰¹å¾**ï¼šä½¿ç”¨ Adagrad
- **RNN/LSTMè®­ç»ƒ**ï¼šä½¿ç”¨ RMSprop
- **ç†è®ºä¿è¯æ”¶æ•›**ï¼šä½¿ç”¨ SGD
- **å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä½¿ç”¨ Adamï¼ˆå‚æ•°æœåŠ¡å™¨æ¶æ„ï¼‰

**è°ƒè¯•å»ºè®®**ï¼š
- å¦‚æœ Adam ä¸æ”¶æ•›ï¼Œå°è¯•é™ä½å­¦ä¹ ç‡åˆ° 0.0001
- å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œè€ƒè™‘ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
- å¦‚æœè¿‡æ‹Ÿåˆä¸¥é‡ï¼Œè€ƒè™‘æƒé‡è¡°å‡ï¼ˆweight decayï¼‰
- ç›‘æ§éªŒè¯é›†æ€§èƒ½ï¼Œè€Œéä»…è®­ç»ƒæŸå¤±

---

### äºŒã€å£è¯­åŒ–ç­”æ¡ˆ

#### ä¸­æ–‡ç‰ˆæœ¬
ä¼˜åŒ–å™¨æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç»„ä»¶ï¼Œå†³å®šäº†è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚SGD æœ€ç®€å•ï¼Œç›´æ¥ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°ï¼Œæ”¶æ•›æ…¢ä½†ç¨³å®šï¼›Momentum åŠ äº†é€Ÿåº¦é¡¹ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘æŒ¯è¡ï¼›Adagrad ä¸ºæ¯ä¸ªå‚æ•°è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ•°æ®ä½†å­¦ä¹ ç‡ä¼šè¡°å‡åˆ°é›¶ï¼›RMSprop ç”¨ç§»åŠ¨å¹³å‡æ”¹è¿› Adagradï¼Œè®­ç»ƒæ›´ç¨³å®šï¼›Adam ç»“åˆäº† Momentum å’Œ RMSprop çš„ä¼˜ç‚¹ï¼Œç”¨ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡å¹¶åšåå·®æ ¡æ­£ï¼Œæ˜¯ç›®å‰æœ€å—æ¬¢è¿çš„ä¼˜åŒ–å™¨ã€‚

**å¿«é€Ÿé€‰æ‹©æŒ‡å—**ï¼š
- æ–°æ‰‹/é»˜è®¤é€‰æ‹©ï¼šAdamï¼ˆå­¦ä¹ ç‡ 0.001ï¼‰
- è¿½æ±‚æœ€å¥½æ€§èƒ½ï¼šAdam æˆ– RMSprop
- è®¡ç®—èµ„æºæœ‰é™ï¼šSGD + Momentum
- ç¨€ç–æ•°æ®ï¼šAdagrad
- RNNè®­ç»ƒï¼šRMSprop

#### English Version
Optimizers are crucial for deep learning training efficiency. SGD is simplest, updates parameters directly with gradients, converges slowly but stably. Momentum adds velocity term for faster convergence and reduced oscillation. Adagrad adapts learning rate per parameter, good for sparse data but learning rate decays to zero. RMSprop improves Adagrad with moving averages for more stable training. Adam combines Momentum and RMSprop advantages using first and second moment estimates with bias correction, currently the most popular optimizer.

**Quick Selection Guide**:
- Beginner/Default: Adam (lr=0.001)
- Best performance: Adam or RMSprop
- Limited compute: SGD + Momentum
- Sparse data: Adagrad
- RNN training: RMSprop

---

## ğŸ“š å¯¼èˆª

â† [ä¸Šä¸€é¢˜: ä¸ºä»€ä¹ˆéœ€è¦éçº¿æ€§æ¿€æ´»å‡½æ•°](12_nonlinear_activation_necessity.md)

â†’ [ä¸‹ä¸€é¢˜: Batchå’ŒSGDçš„ä¼˜ç¼ºç‚¹](14_batch_vs_sgd.md)